{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load, explore and preprocess\n",
    "1. shape and size view\n",
    "2. nan data check\n",
    "3. class similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# loading\n",
    "X_train = np.load('dataset/Assignment2Data/X_train.npy')\n",
    "X_test = np.load('dataset/Assignment2Data/X_test.npy')\n",
    "y_train = np.load('dataset/Assignment2Data/y_train.npy')\n",
    "y_test = np.load('dataset/Assignment2Data/y_test.npy')\n",
    "\n",
    "# data size and shape\n",
    "print(f\"Train size: {X_train.shape[0]}\\nTest size: {X_test.shape[0]}\\nImage shape: {X_test.shape[1:]}\\n\")\n",
    "\n",
    "# Check nan values\n",
    "def check_nan(data) -> bool:\n",
    "    return np.isnan(data).any()\n",
    "\n",
    "print(f\"Nan value exists detect\\nX_train: {check_nan(X_train)}\\ny_train: {check_nan(y_train)}\\nX_test: {check_nan(X_test)}\\ny_test: {check_nan(y_test)}\\n\")\n",
    "\n",
    "# num of class\n",
    "cnt_class = Counter(y_train)\n",
    "print(f\"num of class: {cnt_class}\")\n",
    "classes = list(cnt_class.keys())\n",
    "counts = list(cnt_class.values())\n",
    "\n",
    "# visual of class\n",
    "plt.bar(classes, counts)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(classes)\n",
    "plt.show()\n",
    "\n",
    "# single image show\n",
    "image_index = 3\n",
    "image = X_train[image_index]\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f\"Label: {y_train[image_index]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess\n",
    "1. label one-hot encode (for FNN and CNN)\n",
    "2. train-valid split and data reshape (for FNN and CNN)\n",
    "'''\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train_re = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val_re = X_val.reshape(-1, 28, 28, 1)\n",
    "X_test_re = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "y_train_encoded = to_categorical(y_train, 11)\n",
    "y_val_encoded = to_categorical(y_val, 11)\n",
    "y_test_encoded = to_categorical(y_test, 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Extract feature (for LinearSVC & similarity review)\n",
    "# This cnn model is set just to extract feature for linearSVC and feature similarity review. it s not relevent to the MLP and CNN experiment.\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "all_features = model.predict(X_train)\n",
    "all_features_test = model.predict(X_test)\n",
    "\n",
    "# average feature for each class\n",
    "feature_dict = {key:[] for key in cnt_class.keys()}\n",
    "class_avg_feature = {key:None for key in cnt_class.keys()}\n",
    "\n",
    "for idx, feature in enumerate(all_features):\n",
    "    feature_dict[y_train[idx]].append(feature)\n",
    "\n",
    "for key, features in feature_dict.items():\n",
    "    class_avg_feature[key] = np.mean(features, axis=0)\n",
    "\n",
    "# calculate similarity\n",
    "similarity_matrics = np.zeros((11, 11))\n",
    "\n",
    "sigma = 1.0 # similarity function param\n",
    "\n",
    "for i in range(11):\n",
    "    for j in range(11):\n",
    "        distance = np.linalg.norm(class_avg_feature[i] - class_avg_feature[j])\n",
    "        similarity_matrics[i][j] = 1 / distance\n",
    "\n",
    "# hot map visual\n",
    "plt.imshow(similarity_matrics, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Similarity Matrix Heatmap')\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build\n",
    "1. Linear SVC\n",
    "2. MLP\n",
    "3. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Linear SVC  \n",
    "model  \n",
    "params search  \n",
    "best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "\n",
    "SVC_clf = LinearSVC(dual=\"auto\", random_state=RANDOM_STATE, tol=1e-3, C=1.0, verbose=True)\n",
    "\n",
    "# training\n",
    "SVC_clf.fit(all_features, y_train)\n",
    "\n",
    "# Assessment\n",
    "# accuracy, precision, recall, f1-score\n",
    "y_pred = SVC_clf.predict(all_features_test)\n",
    "\n",
    "acc_SVC = accuracy_score(y_test, y_pred)\n",
    "pre_SVC = precision_score(y_test, y_pred, average='macro')\n",
    "rec_SVC = recall_score(y_test, y_pred, average='macro')\n",
    "f1_SVC = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "cm_SVC = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Acc: {acc_SVC}\\nPre: {pre_SVC}\\nRec: {rec_SVC}\\nF1: {f1_SVC}\\n\")\n",
    "print(f\"Confusion Matrix(SVC): {cm_SVC}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Params search\n",
    "'''\n",
    "# params grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1], \n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'loss': ['squared_hinge'], \n",
    "    'dual': ['auto'],\n",
    "    'tol': [1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "# grid search\n",
    "linear_svc = LinearSVC(random_state=RANDOM_STATE)\n",
    "\n",
    "grid_search = GridSearchCV(linear_svc, param_grid, cv=3, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(all_features, y_train)\n",
    "\n",
    "# best params set\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# best model\n",
    "best_linear_svc = grid_search.best_estimator_\n",
    "test_accuracy = best_linear_svc.score(all_features_test, y_test)\n",
    "print(\"Test set accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual of LinearSVC params search\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "param_names = grid_search.param_grid.keys()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(param_names), ncols=1, figsize=(10, 5 * len(param_names)))\n",
    "\n",
    "for i, param_name in enumerate(param_names):\n",
    "    param_values = results[param_name].unique()\n",
    "    \n",
    "    for value in param_values:\n",
    "        subset = results[results[param_name] == value]\n",
    "        axes[i].plot(subset['param_C'], subset['mean_test_score'], marker='o', label=f'{param_name}={value}')\n",
    "    \n",
    "    axes[i].set_title(f'Grid Search Results for {param_name}')\n",
    "    axes[i].set_xlabel(param_name)\n",
    "    axes[i].set_ylabel('Mean Test Score')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Data Enhance (for both MLP and CNN)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,        \n",
    "    # width_shift_range=0.2,    \n",
    "    # height_shift_range=0.2,\n",
    "    # featurewise_std_normalization=True,        \n",
    "    # zoom_range=0.2,                 \n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(X_train_re, y_train_encoded, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.MLP  \n",
    "model  \n",
    "params search  \n",
    "best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Training setting (for MLP)\n",
    "train_size = 15142\n",
    "val_size = 3786\n",
    "test_size = 4732\n",
    "batch_size = 4\n",
    "epochs = 8\n",
    "lr = 0.01\n",
    "train_steps = train_size // batch_size\n",
    "valid_steps = test_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# MLP model build\n",
    "\n",
    "def FCNN():\n",
    "        '''for single model training'''\n",
    "        FCNN_model = Sequential()\n",
    "        FCNN_model.add(Flatten(input_shape=(28, 28, 1)))  \n",
    "\n",
    "        FCNN_model.add(Dense(128, activation='relu'))\n",
    "        FCNN_model.add(Dropout(0.5))\n",
    "\n",
    "        FCNN_model.add(Dense(64, activation='relu'))\n",
    "        FCNN_model.add(Dropout(0.5))\n",
    "\n",
    "        FCNN_model.add(Dense(11, activation='sigmoid'))\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "        FCNN_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['categorical_accuracy'])\n",
    "        return FCNN_model\n",
    "\n",
    "def FCNN_tuning(hp):\n",
    "        '''for params search'''\n",
    "        model = Sequential()\n",
    "        # flatten\n",
    "        model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "\n",
    "        # 1th dense\n",
    "        model.add(Dense(\n",
    "                units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                activation='relu'))\n",
    "        # drop out\n",
    "        model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # 2th dense\n",
    "        model.add(Dense(\n",
    "                units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                activation='relu'))\n",
    "\n",
    "        # 3th dense output\n",
    "        model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "        # compile\n",
    "        model.compile(\n",
    "                optimizer=Adam(\n",
    "                hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['categorical_accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# MLP train & test\n",
    "# train\n",
    "callbacks = [EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='max')]\n",
    "\n",
    "FCNN_model = FCNN()\n",
    "\n",
    "FCNN_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val_re, y_val_encoded),\n",
    "        validation_steps=valid_steps,\n",
    "        callbacks=callbacks\n",
    "        )\n",
    "\n",
    "# test\n",
    "test_loss, test_accuracy = FCNN_model.evaluate(X_test_re, y_test_encoded, verbose=2)\n",
    "y_pred_MLP = FCNN_model.predict(X_test_re)\n",
    "cm_MLP = confusion_matrix(y_test_encoded, y_pred_MLP)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\\nConfusion Matrix:{cm_MLP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# params tuning\n",
    "FCNN_tuner = RandomSearch(\n",
    "    FCNN_tuning,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='dir_FCNN',\n",
    "    project_name='FCNN')\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./FCNN_logs')\n",
    "\n",
    "FCNN_tuner.search(X_train_re, y_train_encoded, epochs=10, validation_data=(X_test_re, y_test_encoded), callbacks=[tensorboard_callback])\n",
    "\n",
    "best_hps = FCNN_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# model built on best params set\n",
    "FCNN_tuning_model = FCNN_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# train\n",
    "history = FCNN_tuning_model.fit(X_train_re, y_train_encoded, epochs=10, validation_data=(X_test_re, y_test_encoded))\n",
    "\n",
    "# assessment\n",
    "test_loss, test_acc = FCNN_tuning_model.evaluate(X_test_re, y_test_encoded)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "%tensorboard --logdir=./FCNN_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.CNN  \n",
    "model  \n",
    "params search  \n",
    "best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Training setting\n",
    "train_size = 15142\n",
    "val_size = 3786\n",
    "test_size = 4732\n",
    "batch_size = 4\n",
    "epochs = 12\n",
    "lr = 0.001\n",
    "train_steps = int(train_size / batch_size)\n",
    "valid_steps = int(test_size / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "def CNN():\n",
    "    '''for single model training'''\n",
    "    input_shape = (28, 28, 1)\n",
    "\n",
    "    CNN_model = Sequential()\n",
    "\n",
    "    CNN_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN_model.add(Flatten())\n",
    "\n",
    "    CNN_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    CNN_model.add(Dropout(0.5))\n",
    "\n",
    "    CNN_model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "    CNN_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    return CNN_model\n",
    "\n",
    "def CNN_tuning(hp):\n",
    "    '''for params tuning'''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('kernel_size', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('kernel_size', values=[3, 5]),\n",
    "        activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=64, max_value=512, step=64),\n",
    "        activation='relu'))\n",
    "    model.add(Dense(11, activation='softmax'))\n",
    "    \n",
    "    # compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# CNN train & test\n",
    "# train\n",
    "callbacks = [EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='max')]\n",
    "\n",
    "CNN_model = CNN()\n",
    "\n",
    "CNN_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val_re, y_val_encoded),\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=callbacks\n",
    "    )\n",
    "\n",
    "# test\n",
    "test_loss, test_accuracy = CNN_model.evaluate(X_test_re, y_test_encoded, verbose=2)\n",
    "y_pred_CNN = CNN_model.predict(y_test_encoded)\n",
    "cm_CNN = confusion_matrix(y_test_encoded, y_pred_CNN)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\\nConfusion Matrix: {cm_CNN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# params search\n",
    "CNN_tuner = RandomSearch(\n",
    "    CNN_tuning,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='dir_CNN',\n",
    "    project_name='CNN')\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./CNN_logs')\n",
    "\n",
    "CNN_tuner.search(X_train_re, y_train_encoded, epochs=10, validation_data=(X_test_re, y_test_encoded), callbacks=[tensorboard_callback])\n",
    "\n",
    "best_hps = CNN_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# model built on best params set\n",
    "CNN_tuning_model = CNN_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# train\n",
    "history = CNN_tuning_model.fit(X_train_re, y_train_encoded, epochs=10, validation_data=(X_test_re, y_test_encoded))\n",
    "\n",
    "# assessment\n",
    "test_loss, test_acc = CNN_tuning_model.evaluate(X_test_re, y_test_encoded)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "%tensorboard --logdir=./CNN_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
